{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a746370b",
   "metadata": {},
   "source": [
    "##### **Deep Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71be0678",
   "metadata": {},
   "source": [
    "Sure! Here's a mock simple deep learning code example â€” written in pure Python using no external libraries, just to illustrate core concepts like:\n",
    "\n",
    "*   Forward pass\n",
    "*   Simple activation (ReLU or sigmoid)\n",
    "*   Loss calculation (MSE)\n",
    "*   Backpropagation (manually coded)\n",
    "*   Weight update (gradient descent)\n",
    "\n",
    "âš ï¸ Note: This is for educational/demo purposes â€” real deep learning is done with libraries like PyTorch or TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdd9e16",
   "metadata": {},
   "source": [
    "ðŸ§  **Mock Deep Learning Example: Train a Single Neuron to Learn y = 2x + 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5901088d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: [(-5, -9), (-4, -7), (-3, -5), (-2, -3), (-1, -1), (0, 1), (1, 3), (2, 5), (3, 7), (4, 9), (5, 11)]\n",
      "Weight: -0.8689, Bias: -0.28509397937197534, Learning Rate: 0.01\n",
      "Epoch 0: Loss = 365.7803, Weight = 1.8227, Bias = -0.1464\n",
      "Epoch 10: Loss = 166.1227, Weight = 2.1734, Bias = 0.2690\n",
      "Epoch 20: Loss = 165.5371, Weight = 2.1200, Bias = 0.4944\n",
      "Epoch 30: Loss = 165.2569, Weight = 2.0830, Bias = 0.6503\n",
      "Epoch 40: Loss = 165.1229, Weight = 2.0574, Bias = 0.7581\n",
      "Epoch 50: Loss = 165.0588, Weight = 2.0397, Bias = 0.8327\n",
      "Epoch 60: Loss = 165.0281, Weight = 2.0275, Bias = 0.8843\n",
      "Epoch 70: Loss = 165.0135, Weight = 2.0190, Bias = 0.9200\n",
      "Epoch 80: Loss = 165.0064, Weight = 2.0131, Bias = 0.9446\n",
      "Epoch 90: Loss = 165.0031, Weight = 2.0091, Bias = 0.9617\n",
      "\n",
      "=== Testing Trained Model ===\n",
      "x = -2 => y_pred = 0.00 (expected ~ -3)\n",
      "x = 0 => y_pred = 0.97 (expected ~ 1)\n",
      "x = 2 => y_pred = 4.99 (expected ~ 5)\n",
      "x = 4 => y_pred = 9.00 (expected ~ 9)\n"
     ]
    }
   ],
   "source": [
    "# ================\n",
    "# MOCK DEEP LEARNING\n",
    "# ================\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "def relu(x):\n",
    "    return max(0, x)\n",
    "\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return 1 if x > 0 else 0\n",
    "\n",
    "\n",
    "def mean_squared_error(pred, target):\n",
    "    return (pred - target) ** 2\n",
    "\n",
    "\n",
    "def mse_derivative(pred, target):\n",
    "    return 2 * (pred - target)\n",
    "\n",
    "\n",
    "# Our mock training data\n",
    "# Target function: y = 2x + 1\n",
    "training_data = [(x, 2 * x + 1) for x in range(-5, 6)]\n",
    "\n",
    "# Initialize weights and bias\n",
    "weight = random.uniform(-1, 1)\n",
    "bias = random.uniform(-1, 1)\n",
    "learning_rate = 0.01\n",
    "\n",
    "print(f\"Training Data: {training_data}\")\n",
    "print(f\"Weight: {weight:.4f}, Bias: {bias}, Learning Rate: {learning_rate}\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "\n",
    "    for x, y_true in training_data:\n",
    "        # Forward pass\n",
    "        z = weight * x + bias\n",
    "        y_pred = relu(z)\n",
    "\n",
    "        # Loss\n",
    "        loss = mean_squared_error(y_pred, y_true)\n",
    "        total_loss += loss\n",
    "\n",
    "        # Backward pass (manual gradients)\n",
    "        dL_dy = mse_derivative(y_pred, y_true)\n",
    "        dy_dz = relu_derivative(z)\n",
    "        dz_dw = x\n",
    "        dz_db = 1\n",
    "\n",
    "        # Chain rule\n",
    "        dL_dw = dL_dy * dy_dz * dz_dw\n",
    "        dL_db = dL_dy * dy_dz * dz_db\n",
    "\n",
    "        # Gradient descent\n",
    "        weight -= learning_rate * dL_dw\n",
    "        bias -= learning_rate * dL_db\n",
    "\n",
    "    # Print progress\n",
    "    if epoch % 10 == 0:\n",
    "        epoch = f\"Epoch {epoch}: Loss = {total_loss:.4f}, Weight = {weight:.4f}, Bias = {bias:.4f}\"\n",
    "        print(epoch)\n",
    "\n",
    "# Test final model\n",
    "print(\"\\n=== Testing Trained Model ===\")\n",
    "for x in [-2, 0, 2, 4]:\n",
    "    y_pred = relu(weight * x + bias)\n",
    "    print(f\"x = {x} => y_pred = {y_pred:.2f} (expected ~ {2*x + 1})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d857275d",
   "metadata": {},
   "source": [
    "---\n",
    "Weâ€™ll go step-by-step through how gradients are computed and how the model learns using **gradient descent**.\n",
    "\n",
    "#### ðŸŽ¯ GOAL\n",
    "\n",
    "We're training a **single neuron** (i.e., a basic linear model with an activation) to learn this function:\n",
    "\n",
    "```\n",
    "y = 2x + 1\n",
    "```\n",
    "\n",
    "The neuron:\n",
    "\n",
    "* Takes input `x`\n",
    "* Applies a linear function: `z = weight * x + bias`\n",
    "* Passes it through ReLU: `y_pred = relu(z)`\n",
    "* Compares it to the true value `y_true` using MSE loss\n",
    "* Computes gradients of the loss w\\.r.t. `weight` and `bias`\n",
    "* Updates them using gradient descent\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ§® Forward Pass\n",
    "\n",
    "```python\n",
    "z = weight * x + bias\n",
    "y_pred = relu(z)\n",
    "loss = mean_squared_error(y_pred, y_true)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ” Backward Pass (Manual Backpropagation)\n",
    "\n",
    "We want to compute how much the **loss** changes with respect to each parameter:\n",
    "\n",
    "* `dL/dw` (change in loss with respect to weight)\n",
    "* `dL/db` (change in loss with respect to bias)\n",
    "\n",
    "Weâ€™ll do this using the **chain rule** of calculus.\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”„ 1. Loss Derivative (MSE)\n",
    "\n",
    "```python\n",
    "dL_dy = mse_derivative(y_pred, y_true)  # dL/dy\n",
    "```\n",
    "\n",
    "For MSE:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = (y_{\\text{pred}} - y_{\\text{true}})^2  \n",
    "\\Rightarrow \\frac{dL}{dy_{\\text{pred}}} = 2 \\cdot (y_{\\text{pred}} - y_{\\text{true}})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”„ 2. ReLU Derivative\n",
    "\n",
    "```python\n",
    "dy_dz = relu_derivative(z)  # dy/dz\n",
    "```\n",
    "\n",
    "For ReLU:\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(z) = \\max(0, z)  \n",
    "\\Rightarrow \\frac{dy}{dz} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } z > 0 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”„ 3. Derivatives of z\n",
    "\n",
    "```python\n",
    "dz_dw = x\n",
    "dz_db = 1\n",
    "```\n",
    "\n",
    "Because:\n",
    "\n",
    "* $z = w \\cdot x + b$\n",
    "* $\\frac{dz}{dw} = x$\n",
    "* $\\frac{dz}{db} = 1$\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”— Combine with Chain Rule\n",
    "\n",
    "```python\n",
    "dL_dw = dL_dy * dy_dz * dz_dw\n",
    "dL_db = dL_dy * dy_dz * dz_db\n",
    "```\n",
    "\n",
    "This follows:\n",
    "\n",
    "$$\n",
    "\\frac{dL}{dw} = \\frac{dL}{dy} \\cdot \\frac{dy}{dz} \\cdot \\frac{dz}{dw}  \n",
    "= (2 \\cdot (y_{\\text{pred}} - y_{\\text{true}})) \\cdot \\text{ReLU}'(z) \\cdot x\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ§  Weight Update (Gradient Descent)\n",
    "\n",
    "```python\n",
    "weight -= learning_rate * dL_dw\n",
    "bias   -= learning_rate * dL_db\n",
    "```\n",
    "\n",
    "You move **against the gradient** to reduce the loss.\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ“Œ Complete Backprop Code Block in Context:\n",
    "\n",
    "```python\n",
    "# Forward pass\n",
    "z = weight * x + bias\n",
    "y_pred = relu(z)\n",
    "\n",
    "# Loss\n",
    "loss = mean_squared_error(y_pred, y_true)\n",
    "total_loss += loss\n",
    "\n",
    "# Backward pass\n",
    "dL_dy = mse_derivative(y_pred, y_true)    # âˆ‚L/âˆ‚y\n",
    "dy_dz = relu_derivative(z)                # âˆ‚y/âˆ‚z\n",
    "dz_dw = x                                 # âˆ‚z/âˆ‚w\n",
    "dz_db = 1                                 # âˆ‚z/âˆ‚b\n",
    "\n",
    "dL_dw = dL_dy * dy_dz * dz_dw             # âˆ‚L/âˆ‚w\n",
    "dL_db = dL_dy * dy_dz * dz_db             # âˆ‚L/âˆ‚b\n",
    "\n",
    "# Update weights\n",
    "weight -= learning_rate * dL_dw\n",
    "bias   -= learning_rate * dL_db\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### âœ… Summary\n",
    "\n",
    "| Step                  | What It Does                           |\n",
    "| --------------------- | -------------------------------------- |\n",
    "| Forward Pass          | Compute prediction `y_pred` and loss   |\n",
    "| Loss Derivative       | Compute how loss changes w\\.r.t output |\n",
    "| Activation Derivative | Account for ReLU turning off gradients |\n",
    "| Chain Rule            | Propagate error to weight and bias     |\n",
    "| Update                | Apply gradients via gradient descent   |\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to:\n",
    "\n",
    "* Extend this to a **hidden layer**?\n",
    "* Switch to **sigmoid** or **no activation**?\n",
    "* Visualize how loss decreases over time?\n",
    "\n",
    "Let me know how deep you want to go!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300611e6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "artificial-intelligence_1.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
