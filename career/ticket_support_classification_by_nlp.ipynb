{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6da52586",
   "metadata": {},
   "source": [
    "##### **2. NLP-Based Support Ticket Classification (TensorFlow, NLTK)**\n",
    "* Ticket Classification\n",
    "    * Preprocessed customer support ticket data and built an LSTM-based neural network.\n",
    "    * Implemented text vectorization using TF-IDF and Word2Vec; achieved 87% F1 score.\n",
    "    * Integrated the model with a basic Flask API for demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d009d94",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "264d43ad",
   "metadata": {},
   "source": [
    "Here‚Äôs a **complete example** for your project:\n",
    "\n",
    "> **2. NLP-Based Support Ticket Classification (TensorFlow, NLTK)**\n",
    "> Using **LSTM**, **TF-IDF / Word2Vec**, and **Streamlit** for demo\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Project Overview\n",
    "\n",
    "| Step          | Tool                                        |\n",
    "| ------------- | ------------------------------------------- |\n",
    "| Preprocessing | NLTK                                        |\n",
    "| Vectorization | TF-IDF (for this example)                   |\n",
    "| Model         | LSTM with TensorFlow/Keras                  |\n",
    "| Interface     | Streamlit                                   |\n",
    "| Evaluation    | F1 Score                                    |\n",
    "| Dataset       | Simulated support tickets (`text`, `label`) |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Folder Structure\n",
    "\n",
    "```\n",
    "nlp_ticket_classifier/\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ tickets.csv\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ model/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ lstm_model.h5\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ tokenizer.pkl\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ label_encoder.pkl\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ train_model.py          # Model training script\n",
    "‚îú‚îÄ‚îÄ app.py                  # Streamlit app\n",
    "‚îî‚îÄ‚îÄ requirements.txt\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìÅ Step 1: Sample Dataset (`data/tickets.csv`)\n",
    "\n",
    "```csv\n",
    "text,label\n",
    "Internet not working,Technical Support\n",
    "Unable to login,Technical Support\n",
    "Want to change my plan,Billing\n",
    "Need help with charges,Billing\n",
    "Cancel my subscription,Account\n",
    "Update my email,Account\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Step 2: `train_model.py`\n",
    "\n",
    "```python\n",
    "# train_model.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('data/tickets.csv')\n",
    "\n",
    "# Clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text.lower())\n",
    "    words = text.split()\n",
    "    return ' '.join([word for word in words if word not in stop_words])\n",
    "\n",
    "df['clean_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['label_enc'] = label_encoder.fit_transform(df['label'])\n",
    "y = to_categorical(df['label_enc'])\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(df['clean_text'])\n",
    "X_seq = tokenizer.texts_to_sequences(df['clean_text'])\n",
    "X = pad_sequences(X_seq, maxlen=20)\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=64, input_length=20),\n",
    "    LSTM(64),\n",
    "    Dense(y.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=4, validation_split=0.1)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Save model\n",
    "model.save('model/lstm_model.h5')\n",
    "\n",
    "with open('model/tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "with open('model/label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üñ•Ô∏è Step 3: `app.py` (Streamlit App)\n",
    "\n",
    "```python\n",
    "# app.py\n",
    "\n",
    "import streamlit as st\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load model and preprocessors\n",
    "model = tf.keras.models.load_model('model/lstm_model.h5')\n",
    "\n",
    "with open('model/tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "with open('model/label_encoder.pkl', 'rb') as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "# Clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text.lower())\n",
    "    words = text.split()\n",
    "    return ' '.join([word for word in words if word not in stop_words])\n",
    "\n",
    "# Streamlit UI\n",
    "st.title(\"üõ†Ô∏è Support Ticket Classifier\")\n",
    "st.write(\"Enter a customer support ticket message to classify it.\")\n",
    "\n",
    "user_input = st.text_area(\"Ticket Text:\")\n",
    "\n",
    "if st.button(\"Classify\"):\n",
    "    cleaned = clean_text(user_input)\n",
    "    seq = tokenizer.texts_to_sequences([cleaned])\n",
    "    padded = pad_sequences(seq, maxlen=20)\n",
    "    pred = model.predict(padded)\n",
    "    label = label_encoder.inverse_transform([np.argmax(pred)])\n",
    "    st.success(f\"Predicted Category: **{label[0]}**\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ `requirements.txt`\n",
    "\n",
    "```txt\n",
    "streamlit\n",
    "tensorflow\n",
    "nltk\n",
    "pandas\n",
    "scikit-learn\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ To Run the App:\n",
    "\n",
    "1. Install dependencies:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "2. Train the model:\n",
    "\n",
    "```bash\n",
    "python train_model.py\n",
    "```\n",
    "\n",
    "3. Run the Streamlit app:\n",
    "\n",
    "```bash\n",
    "streamlit run app.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like:\n",
    "\n",
    "* To use **Word2Vec** instead of TF-IDF/Tokenizer\n",
    "* Deploy the Streamlit app (e.g., with Docker or Streamlit Cloud)\n",
    "* Add confidence scores or multi-class support\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b7cd5e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
