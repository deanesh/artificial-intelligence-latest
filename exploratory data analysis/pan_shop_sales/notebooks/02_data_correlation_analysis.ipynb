{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fec9bcf",
   "metadata": {},
   "source": [
    "#### Data Columns - Breakup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1dbb00",
   "metadata": {},
   "source": [
    "\n",
    " Data : Pan Shop Sales Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0658f952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-04 02:49:35 | 02_data_correlation_analysis.ipynb | 🔄 Starting data correlation analysis...\n",
      "2025-09-04 02:49:35 | data_utils.py | 🚀 Fetching data frame from partial file name\n",
      "2025-09-04 02:49:35 | data_utils.py | 📄 Matched file: cleaned_pan_shop_and_sales_20250903_223450.csv\n",
      "2025-09-04 02:49:35 | data_utils.py | 📥 Reading CSV: ../../../cleaned_data_files/cleaned_pan_shop_and_sales_20250903_223450.csv\n",
      "2025-09-04 02:49:35 | data_utils.py | ✅ Loaded CSV with shape: (7736, 17)\n",
      "2025-09-04 02:49:35 | data_utils.py | ⏱️ Time taken to read CSV: 0.03 seconds\n",
      "2025-09-04 02:49:35 | data_utils.py | ✅ Fetching DataFrame completed in 0.03 seconds\n",
      "2025-09-04 02:49:35 | data_utils.py | 🚀 Fetching cat and cont columns from dataframe \n",
      "2025-09-04 02:49:35 | data_utils.py | 🚀 Fetched cat and cont columns from dataframe \n",
      "2025-09-04 02:49:35 | 02_data_correlation_analysis.ipynb |  Categorical Columns : ['name', 'timings', 'owner', 'location', 'city', 'state', 'date', 'time', 'item', 'item_description', 'packed']\n",
      "2025-09-04 02:49:35 | 02_data_correlation_analysis.ipynb |  Continuous Columns : ['size', 'rent', 'quantity', 'sold', 'unitprice', 'profit']\n",
      "2025-09-04 02:49:35 | correlation_utils.py | 🚀 Fetching important continuous columns from dataframe for features list\n",
      "2025-09-04 02:49:35 | correlation_utils.py | ✅ Fetched important continuous columns from dataframe for features list\n",
      "2025-09-04 02:49:35 | 02_data_correlation_analysis.ipynb | ✅ Important continuous columns (corr > 0.4):\n",
      "2025-09-04 02:49:35 | 02_data_correlation_analysis.ipynb |    - sold: 0.69\n",
      "2025-09-04 02:49:35 | 02_data_correlation_analysis.ipynb |    - unitprice: 0.45\n",
      "2025-09-04 02:49:35 | 02_data_correlation_analysis.ipynb |    - quantity: 0.45\n",
      "2025-09-04 02:49:35 | correlation_utils.py | 🚀 Fetching important categorical columns for target: profit\n",
      "2025-09-04 02:49:38 | correlation_utils.py | ✅ Completed categorical feature analysis in 2.87 secs\n",
      "2025-09-04 02:49:38 | 02_data_correlation_analysis.ipynb | ✅ Important categorical features (Cramér’s V ≥ 0.2):\n",
      "2025-09-04 02:49:38 | 02_data_correlation_analysis.ipynb |    - time: 0.94\n",
      "2025-09-04 02:49:38 | 02_data_correlation_analysis.ipynb |    - packed: 0.58\n",
      "2025-09-04 02:49:38 | 02_data_correlation_analysis.ipynb |    - date: 0.58\n",
      "2025-09-04 02:49:38 | 02_data_correlation_analysis.ipynb |    - item: 0.58\n",
      "2025-09-04 02:49:38 | 02_data_correlation_analysis.ipynb |    - name: 0.58\n",
      "2025-09-04 02:49:38 | 02_data_correlation_analysis.ipynb |    - item_description: 0.58\n",
      "2025-09-04 02:49:38 | 02_data_correlation_analysis.ipynb |    - location: 0.32\n",
      "2025-09-04 02:49:38 | 02_data_correlation_analysis.ipynb |    - owner: 0.32\n",
      "2025-09-04 02:49:38 | 02_data_correlation_analysis.ipynb | Imp Continuous columns list : ['sold', 'unitprice', 'quantity']\n",
      "2025-09-04 02:49:38 | 02_data_correlation_analysis.ipynb | Imp categorical columns list : ['time', 'packed', 'date', 'item', 'name', 'item_description', 'location', 'owner']\n",
      "2025-09-04 02:49:38 | 02_data_correlation_analysis.ipynb | Imp features columns list : ['unitprice', 'name', 'owner', 'sold', 'time', 'date', 'packed', 'quantity', 'location', 'item_description', 'item']\n",
      "   time_target_enc  packed_target_enc  date_target_enc  item_target_enc  \\\n",
      "0       480.350000         200.057560       191.364262       205.328132   \n",
      "1       301.590000         199.493595       172.119301       229.192046   \n",
      "2       202.197049         199.493595       149.515323       200.824888   \n",
      "3       102.710000         196.959450       180.036533       177.719955   \n",
      "4       202.197049         196.959450       202.609135       192.991639   \n",
      "\n",
      "   name_target_enc  item_description_target_enc  location_target_enc  \\\n",
      "0       170.676829                   202.930290           192.553500   \n",
      "1       224.271750                   195.712712           234.456471   \n",
      "2       224.271750                   195.712712           234.456471   \n",
      "3       198.909744                   193.475715           189.442381   \n",
      "4       198.909744                   193.475715           189.442381   \n",
      "\n",
      "   owner_target_enc  \n",
      "0        192.553500  \n",
      "1        234.456471  \n",
      "2        234.456471  \n",
      "3        189.442381  \n",
      "4        189.442381  \n",
      "   size   rent  quantity  unitprice  sold  time_target_enc  name_target_enc  \\\n",
      "0    74  11914        31      48.55    30       480.350000       170.676829   \n",
      "1    74  11914        76      33.71    66       301.590000       224.271750   \n",
      "2    74  11914        31      34.50     6       202.197049       224.271750   \n",
      "3    74  11914        59      31.67    14       102.710000       198.909744   \n",
      "4    74  11914        78      36.25    56       202.197049       198.909744   \n",
      "\n",
      "   owner_target_enc  location_target_enc  \n",
      "0        192.553500           192.553500  \n",
      "1        234.456471           234.456471  \n",
      "2        234.456471           234.456471  \n",
      "3        189.442381           189.442381  \n",
      "4        189.442381           189.442381  \n",
      "0    480.35\n",
      "1    301.59\n",
      "2     21.07\n",
      "3    102.71\n",
      "4    476.62\n",
      "Name: profit, dtype: float64\n",
      "RMSE: 82.0035\n",
      "R^2 Score: 0.8820\n",
      "2025-09-04 02:49:43 | 02_data_correlation_analysis.ipynb | Current Working Dir: e:\\Git-Repos\\artificial-intelligence-latest\\exploratory data analysis\\pan_shop_sales\\notebooks\n",
      "2025-09-04 02:49:43 | 02_data_correlation_analysis.ipynb | Util Functions Dir: ../../../utility functions\n",
      "2025-09-04 02:49:43 | 02_data_correlation_analysis.ipynb | Data File Dir: ../../../cleaned_data_files/\n",
      "2025-09-04 02:49:43 | 02_data_correlation_analysis.ipynb | 🔄 Completed data correlation analysis...\n",
      "2025-09-04 02:49:43 | 02_data_correlation_analysis.ipynb | ✅ Total time taken: 7.66 seconds\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "NOTEBOOK_NAME = \"02_data_correlation_analysis.ipynb\"\n",
    "\n",
    "# Start Time\n",
    "start_time = time.time()\n",
    "\n",
    "curr_dir = os.getcwd()\n",
    "util_functions_dir = \"../../../utility functions\"\n",
    "# Common Utilities (data_utils) Path\n",
    "UTILITY_FUNCTIONS_DIR_PATH = os.path.abspath(os.path.join(curr_dir, util_functions_dir))\n",
    "if UTILITY_FUNCTIONS_DIR_PATH not in sys.path:\n",
    "    sys.path.append(UTILITY_FUNCTIONS_DIR_PATH)\n",
    "\n",
    "\n",
    "from data_utils import get_dataframe_by_partial_file_name, get_cat_and_con_cols_list\n",
    "from log_utils import log, set_log_source\n",
    "from correlation_utils import imp_cat_cols, get_imp_con_cols\n",
    "\n",
    "# Set the notebook name globally once\n",
    "set_log_source(NOTEBOOK_NAME)\n",
    "\n",
    "log(\"🔄 Starting data correlation analysis...\")\n",
    "\n",
    "# As always cleaned data exists in below folder only hard coding the value\n",
    "data_dir = \"../../../cleaned_data_files/\"\n",
    "partial_file_name = \"cleaned_pan_shop_and\"\n",
    "\n",
    "# DataFrame for correlation analysis\n",
    "df = get_dataframe_by_partial_file_name(data_dir, partial_file_name)\n",
    "\n",
    "# Target\n",
    "target = \"profit\"\n",
    "if target not in df.columns:\n",
    "    log(f\"❌ Target column '{target}' not found in the DataFrame!\")\n",
    "    print(f\"[ERROR] Target column '{target}' not found!\", file=sys.stderr)\n",
    "    sys.exit(1)\n",
    "\n",
    "# Categorical and Continuous Columns\n",
    "cat_cols, con_cols = get_cat_and_con_cols_list(df)\n",
    "log(f\" Categorical Columns : {cat_cols}\")\n",
    "log(f\" Continuous Columns : {con_cols}\")\n",
    "\n",
    "\n",
    "corr_threshold = 0.4\n",
    "\n",
    "# Fetch important continuous columns with correlation > threshold\n",
    "imp_con_cols_list = []\n",
    "\n",
    "imp_con_corr = get_imp_con_cols(df, target, corr_threshold)\n",
    "\n",
    "if not imp_con_corr.empty:\n",
    "    log(f\"✅ Important continuous columns (corr > {corr_threshold}):\")\n",
    "    for col, val in imp_con_corr.items():\n",
    "        imp_con_cols_list.append(col)\n",
    "        log(f\"   - {col}: {val:.2f}\")\n",
    "else:\n",
    "    log(f\"⚠️ No cont features found with corr > {corr_threshold} to target: '{target}'\")\n",
    "\n",
    "\n",
    "# Fetch important categorical columns with p-val >= threshold\n",
    "imp_cat_cols_list = []\n",
    "p_val_threshold = 0.2\n",
    "cramers_df = imp_cat_cols(df, cat_cols, target, p_val_threshold)\n",
    "\n",
    "if not cramers_df.empty:\n",
    "    log(f\"✅ Important categorical features (Cramér’s V ≥ {p_val_threshold}):\")\n",
    "    for _, row in cramers_df.iterrows():\n",
    "        imp_cat_cols_list.append(row[\"Feature\"])\n",
    "        log(f\"   - {row['Feature']}: {row['Cramers_V']:.2f}\")\n",
    "else:\n",
    "    log(f\"⚠️ No categorical features with Cramér’s V ≥ {p_val_threshold}\")\n",
    "\n",
    "log(f\"Imp Continuous columns list : {imp_con_cols_list}\")\n",
    "log(f\"Imp categorical columns list : {imp_cat_cols_list}\")\n",
    "\n",
    "features_list = list(set(imp_cat_cols_list + imp_con_cols_list))\n",
    "log(f\"Imp features columns list : {features_list}\")\n",
    "\n",
    "\n",
    "def target_encode(train_series, target_series, n_splits=5):\n",
    "    # Initialize encoded feature with zeros\n",
    "    encoded_feature = pd.Series(index=train_series.index, dtype=float)\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    for train_idx, val_idx in kf.split(train_series):\n",
    "        # Calculate mean target per category on train fold\n",
    "        means = (\n",
    "            target_series.iloc[train_idx].groupby(train_series.iloc[train_idx]).mean()\n",
    "        )\n",
    "        # Map means to validation fold\n",
    "        encoded_feature.iloc[val_idx] = train_series.iloc[val_idx].map(means)\n",
    "\n",
    "    # For any missing mappings (new categories), fill with global mean\n",
    "    encoded_feature.fillna(target_series.mean(), inplace=True)\n",
    "\n",
    "    return encoded_feature\n",
    "\n",
    "\n",
    "# Apply target encoding to each categorical column\n",
    "for col in imp_cat_cols_list:\n",
    "    df[col + \"_target_enc\"] = target_encode(df[col], df[target])\n",
    "\n",
    "print(df[[col + \"_target_enc\" for col in imp_cat_cols_list]].head())\n",
    "\n",
    "# Select continuous features\n",
    "cont_features = [\"size\", \"rent\", \"quantity\", \"unitprice\", \"sold\"]\n",
    "\n",
    "# Select encoded categorical features\n",
    "encoded_cat_features = [\n",
    "    col + \"_target_enc\" for col in [\"time\", \"name\", \"owner\", \"location\"]\n",
    "]\n",
    "# Final feature list\n",
    "final_features = cont_features + encoded_cat_features\n",
    "\n",
    "# Prepare X and y\n",
    "X = df[final_features]\n",
    "y = df[\"profit\"]\n",
    "\n",
    "print(X.head())\n",
    "print(y.head())\n",
    "\n",
    "\n",
    "# Features and target\n",
    "X = df[cont_features + encoded_cat_features]\n",
    "y = df[\"profit\"]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Model\n",
    "model = RandomForestRegressor(random_state=42, n_estimators=100)\n",
    "\n",
    "# Train\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test, y_pred)  # by default, squared=True (MSE)\n",
    "rmse = np.sqrt(mse)  # take square root to get RMSE\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R^2 Score: {r2_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "log(f\"Current Working Dir: {curr_dir}\")\n",
    "log(f\"Util Functions Dir: {util_functions_dir}\")\n",
    "log(f\"Data File Dir: {data_dir}\")\n",
    "\n",
    "\n",
    "# End Time\n",
    "end_time = time.time()\n",
    "log(\"🔄 Completed data correlation analysis...\")\n",
    "log(f\"✅ Total time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b4cdd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "artificial-intelligence_1.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
