{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d412e173",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "‚úÖ What This Code Does Well\n",
    "\n",
    "* Document and Query Embeddings:\n",
    "    * Mocked 3 documents with distinct orthogonal embeddings.\n",
    "    * The query embedding is close to the third document, which makes testing predictable\n",
    "\n",
    "* Cosine Similarity for Retrieval:\n",
    "    * Uses cosine_similarity from sklearn.metrics.pairwise ‚Äî a solid choice for comparing vector similarities\n",
    "\n",
    "* Retrieval Logic:\n",
    "    * Computes cosine similarity for each document.\n",
    "    * Uses np.argmax to find the most relevant document ‚Äî simple and effective.\n",
    "\n",
    "* Response Generation:\n",
    "    * A mocked generation step that nicely mimics how a language model might condition on a retrieved context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b067a0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Index: 2\n",
      "üîç Query: Tell me about NLP models\n",
      "üìÑ Retrieved Document: Transformers are deep learning models used in NLP.\n",
      "ü§ñ Generated Response: Based on the retrieved document: 'Transformers are deep learning models used in NLP.', here's an answer about your query: NLP uses models like transformers for language tasks.\n",
      "(2, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Step 1: Mock \"documents\" and their \"embeddings\"\n",
    "documents = [\n",
    "    {\n",
    "        \"text\": \"Python is a popular programming language for AI.\",\n",
    "        \"embedding\": np.array([1, 0, 0]),\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Machine learning involves training models on data.\",\n",
    "        \"embedding\": np.array([0, 1, 0]),\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Transformers are deep learning models used in NLP.\",\n",
    "        \"embedding\": np.array([0, 0, 1]),\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "# Step 2: Mock query and its \"embedding\"\n",
    "query_embedding = np.array([0, 0, 0.85])\n",
    "\n",
    "\n",
    "# Step 3: Retrieve relevant document using cosine similarity\n",
    "def retrieve_top_document(query_embedding, documents):\n",
    "    similarities = [\n",
    "        cosine_similarity([query_embedding], [document[\"embedding\"]])[0][0]\n",
    "        for document in documents\n",
    "    ]\n",
    "    top_index = np.argmax(similarities)\n",
    "    print(f\"Top Index: {top_index}\")\n",
    "    return documents[top_index]\n",
    "\n",
    "\n",
    "top_doc = retrieve_top_document(query_embedding, documents)\n",
    "\n",
    "\n",
    "# Step 4: Mock generation (simulate response using the document)\n",
    "def generate_response(query, context):\n",
    "    return f\"Based on the retrieved document: '{context}', here's an answer about your query: NLP uses models like transformers for language tasks.\"\n",
    "\n",
    "\n",
    "# Step 5: Output the result\n",
    "\n",
    "query = \"Tell me about NLP models\"\n",
    "print(\"üîç Query:\", query)\n",
    "response = generate_response(query, top_doc[\"text\"])\n",
    "print(\"üìÑ Retrieved Document:\", top_doc[\"text\"])\n",
    "print(\"ü§ñ Generated Response:\", response)\n",
    "\n",
    "A = np.array([[1, 0], [0, 1]])\n",
    "B = np.array([[1, 1]])\n",
    "\n",
    "similarity = cosine_similarity(A, B)\n",
    "print(similarity.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f06c017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=E:\\Git-Repos\\artificial-intelligence_1.0\\.venv` does not match the project environment path `e:\\Git-Repos\\artificial-intelligence-latest\\.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m169 packages\u001b[0m \u001b[2min 18.46s\u001b[0m\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m transformers \u001b[2m(11.1MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m torch \u001b[2m(230.2MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m sympy \u001b[2m(6.0MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m networkx \u001b[2m(1.9MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m tokenizers \u001b[2m(2.6MiB)\u001b[0m\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m networkx\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m tokenizers\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m sympy\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m transformers\n",
      "  \u001b[31m√ó\u001b[0m Failed to download `torch==2.8.0`\n",
      "\u001b[31m  ‚îú‚îÄ‚ñ∂ \u001b[0mFailed to extract archive: torch-2.8.0-cp313-cp313-win_amd64.whl\n",
      "\u001b[31m  ‚îú‚îÄ‚ñ∂ \u001b[0mI/O operation failed during extraction\n",
      "\u001b[31m  ‚ï∞‚îÄ‚ñ∂ \u001b[0mFailed to download distribution due to network timeout. Try increasing\n",
      "\u001b[31m      \u001b[0mUV_HTTP_TIMEOUT (current value: 30s).\n",
      "\u001b[36m  help: \u001b[0mIf you want to add the package regardless of the failed resolution,\n",
      "        provide the `\u001b[32m--frozen\u001b[39m` flag to skip locking and syncing.\n"
     ]
    }
   ],
   "source": [
    "!(uv add sentence-transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "975b45df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=E:\\Git-Repos\\artificial-intelligence_1.0\\.venv` does not match the project environment path `e:\\Git-Repos\\artificial-intelligence-latest\\.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m136 packages\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m3 packages\u001b[0m \u001b[2min 112ms\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m3 packages\u001b[0m \u001b[2min 1.40s\u001b[0m\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mipykernel\u001b[0m\u001b[2m==6.29.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mipykernel\u001b[0m\u001b[2m==6.30.1\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mjupyter-client\u001b[0m\u001b[2m==7.4.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjupyter-client\u001b[0m\u001b[2m==8.6.3\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mnotebook\u001b[0m\u001b[2m==6.5.7\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnotebook\u001b[0m\u001b[2m==7.4.5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "339f0382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query embedding shape: (384,)\n",
      "First 5 values: [-0.00403581 -0.05510387  0.03066592  0.01598372  0.01516357]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load a pre-trained embedding model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Your query\n",
    "query = \"Tell me about NLP models.\"\n",
    "\n",
    "# Generate the embedding (a high-dimensional vector)\n",
    "query_embedding = model.encode(query)\n",
    "\n",
    "print(\"Query embedding shape:\", query_embedding.shape)\n",
    "print(\"First 5 values:\", query_embedding[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651ec90b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
