{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cb568bd",
   "metadata": {},
   "source": [
    "| **Topic**                     | **Flow**                                                              | **Comments**                                                              | **Links**                                                                      |\n",
    "| ----------------------------- | --------------------------------------------------------------------- | ------------------------------------------------------------------------- | ------------------------------------------------------------------------------ |\n",
    "| **Gen AI Layers**   | Input → Embedding → Transformer Blocks → Output                       | Foundation of LLMs like GPT; generates content token by token             | [OpenAI GPT](https://platform.openai.com)                                      |\n",
    "| **Transformers Architecture** | Input Embedding → Positional Encoding → Self-Attention → FFN → Output | Core of modern NLP models; parallel and context-aware                     | [Attention is All You Need](https://arxiv.org/abs/1706.03762)                  |\n",
    "| **Agentic AI**                 | Goal → Task Breakdown → Tool Use → Action                             | LLMs with planning, memory, and tools; acts autonomously                  | [Auto-GPT](https://github.com/Torantulino/Auto-GPT)                            |\n",
    "| **Artificial Intelligence**                        | Data → Learning → Reasoning/Action                                    | Broad field aiming to replicate human intelligence                        | [Wikipedia - AI](https://en.wikipedia.org/wiki/Artificial_intelligence)        |\n",
    "| **CNN**                       | Input Image → Convolution → Pooling → Flatten → Output                | Best for image and spatial data recognition                               | [CNN Explained](https://cs231n.github.io/convolutional-networks/)              |\n",
    "| **Deep Learning**            | Data → Neural Networks → Feature Extraction → Output                  | Subset of ML; excels with unstructured data (images, audio, text)         | [Deep Learning Book](https://www.deeplearningbook.org/)                        |\n",
    "| **LangChain**                 | Prompt → Chain → Memory/Tools → Response                              | Framework to build LLM-powered apps with modular components               | [LangChain](https://www.langchain.com)                                         |\n",
    "| **ML**         | Data → Training → Model → Prediction                                  | ML learns patterns from data; includes supervised, unsupervised, etc.     | [ML Crash Course](https://developers.google.com/machine-learning/crash-course) |\n",
    "| **NLP**                       | Text → Tokenization → Embedding → Model → Output                      | Language understanding tasks like translation, Q\\&A, summarization        | [NLP Guide](https://huggingface.co/learn/nlp-course)                           |\n",
    "| **RNN**                       | Input → Hidden State Loop → Output                                    | Good for sequences (e.g., time series), but struggles with long memory    | [RNN Tutorial](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)     |\n",
    "| **RAG**                       | Query → Retrieve Docs → Generate Response                             | Combines retrieval (search) with generation for grounded, factual answers | [RAG Paper](https://arxiv.org/abs/2005.11401)                                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606d525a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
