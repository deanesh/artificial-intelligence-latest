{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb7386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Set Streamlit page config\n",
    "st.set_page_config(page_title=\"Local Chat with Mistral\", layout=\"wide\")\n",
    "\n",
    "st.title(\"üß† Local AI Chatbot using Ollama + LangChain\")\n",
    "\n",
    "# Initialize chat memory and model\n",
    "@st.cache_resource\n",
    "def load_conversation():\n",
    "    llm = Ollama(model=\"mistral\")  # Make sure Mistral is pulled with `ollama pull mistral`\n",
    "    memory = ConversationBufferMemory()\n",
    "    convo = ConversationChain(llm=llm, memory=memory)\n",
    "    return convo\n",
    "\n",
    "conversation = load_conversation()\n",
    "\n",
    "# Chat interface\n",
    "user_input = st.chat_input(\"Ask me anything...\")\n",
    "\n",
    "if user_input:\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(user_input)\n",
    "\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        response = conversation.run(user_input)\n",
    "        st.markdown(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4211a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import io\n",
    "\n",
    "# Init local model (make sure Mistral is pulled: `ollama pull mistral`)\n",
    "llm = Ollama(model=\"mistral\")\n",
    "\n",
    "# Global file content\n",
    "extracted_chunks = []\n",
    "\n",
    "# Text splitter for long documents\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# File uploader widget\n",
    "file_uploader = widgets.FileUpload(\n",
    "    accept=\".pdf,.xlsx,.xls\", multiple=False, description=\"üìÅ Upload PDF or Excel\"\n",
    ")\n",
    "\n",
    "# Input field + submit button\n",
    "question_input = widgets.Text(\n",
    "    placeholder=\"Ask a question about the file...\",\n",
    "    description=\"You:\",\n",
    "    layout=widgets.Layout(width=\"75%\"),\n",
    ")\n",
    "\n",
    "submit_button = widgets.Button(\n",
    "    description=\"Ask\", button_style=\"success\", layout=widgets.Layout(width=\"15%\")\n",
    ")\n",
    "\n",
    "# Output area\n",
    "output = widgets.Output()\n",
    "\n",
    "# Display widgets\n",
    "input_row = widgets.HBox([question_input, submit_button])\n",
    "display(file_uploader, input_row, output)\n",
    "\n",
    "\n",
    "# Function: extract file content and chunk it\n",
    "def extract_file_content(change):\n",
    "    global extracted_chunks\n",
    "    extracted_chunks = []\n",
    "\n",
    "    uploaded_files = file_uploader.value\n",
    "    if not uploaded_files:\n",
    "        return\n",
    "\n",
    "    # Handle uploaded file\n",
    "    file_info = uploaded_files[0]\n",
    "    file_name = file_info[\"name\"]\n",
    "    file_type = file_name.split(\".\")[-1].lower()\n",
    "    file_data = file_info[\"content\"]\n",
    "\n",
    "    # Process PDF\n",
    "    if file_type == \"pdf\":\n",
    "        reader = PyPDF2.PdfReader(io.BytesIO(file_data))\n",
    "        text = \"\\n\".join(\n",
    "            page.extract_text() for page in reader.pages if page.extract_text()\n",
    "        )\n",
    "        docs = splitter.split_documents([Document(page_content=text)])\n",
    "\n",
    "    # Process Excel\n",
    "    elif file_type in [\"xlsx\", \"xls\"]:\n",
    "        df = pd.read_excel(io.BytesIO(file_data))\n",
    "        text = df.to_csv(index=False)\n",
    "        docs = splitter.split_documents([Document(page_content=text)])\n",
    "\n",
    "    else:\n",
    "        docs = [Document(page_content=\"‚ùå Unsupported file type.\")]\n",
    "\n",
    "    extracted_chunks = docs\n",
    "    with output:\n",
    "        clear_output()\n",
    "        print(f\"‚úÖ Loaded and processed file: {file_name} ‚Äî {len(docs)} chunks ready.\")\n",
    "        print(\"üìÑ You can now ask questions about the document.\")\n",
    "\n",
    "\n",
    "# Function: handle question submission\n",
    "def on_button_click(b):\n",
    "    query = question_input.value\n",
    "    if not extracted_chunks:\n",
    "        with output:\n",
    "            print(\"‚ö†Ô∏è Please upload a file first.\")\n",
    "        return\n",
    "    if not query.strip():\n",
    "        with output:\n",
    "            print(\"‚ö†Ô∏è Please type a question.\")\n",
    "        return\n",
    "\n",
    "    # Combine all chunks into one prompt (can be optimized later)\n",
    "    full_text = \"\\n\".join(chunk.page_content for chunk in extracted_chunks)\n",
    "    prompt = (\n",
    "        f\"Given the following document:\\n{full_text}\\n\\nAnswer this question:\\n{query}\"\n",
    "    )\n",
    "    response = llm(prompt)\n",
    "\n",
    "    with output:\n",
    "        print(f\"\\nüß† Question: {query}\")\n",
    "        print(f\"ü§ñ Answer: {response}\")\n",
    "    question_input.value = \"\"\n",
    "\n",
    "\n",
    "# Bind event handlers\n",
    "file_uploader.observe(extract_file_content, names=\"value\")\n",
    "submit_button.on_click(on_button_click)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d5839bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d644757457441328d4237ccda8d4095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DEANESH\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\DEANESH\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89efdf760e93451ca326c77970f0588f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6b7d2cb12c4393af21526929d18b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e74987662a99476a92b0d52eadd07289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03568d5578e4a3cae3375b5bc463c5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ba1bf47e5e4d8f825f5b4fcda64bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c72d49ea98e48d7b1896321e661a3fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c479aa95a6c4c64ad3043917f43b031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68bf214e231c4a56bfbccc37a8367315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b67fd6dec0422e993c39eeb251f580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e972a6f58cc4d5f8cccc8da50339a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e606aeec954488b89c6775ff3dbb7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.pdf,.xlsx,.xls', description='üìÅ Upload File')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a26eefb67f4474a3dd92abd763dbc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='', description='You:', layout=Layout(width='75%'), placeholder='Ask a question abou‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda241488d8a48cb9ec94a6acaf51387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import faiss\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "# Local LLM (Ollama with mistral)\n",
    "llm = Ollama(model=\"mistral\")\n",
    "\n",
    "# Embedder (local SentenceTransformers model)\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Small & fast\n",
    "\n",
    "# Vector DB\n",
    "index = None\n",
    "chunk_map = []\n",
    "\n",
    "# Splitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# Widgets\n",
    "file_uploader = widgets.FileUpload(\n",
    "    accept=\".pdf,.xlsx,.xls\", multiple=False, description=\"üìÅ Upload File\"\n",
    ")\n",
    "question_input = widgets.Text(\n",
    "    placeholder=\"Ask a question about the file...\",\n",
    "    description=\"You:\",\n",
    "    layout=widgets.Layout(width=\"75%\"),\n",
    ")\n",
    "submit_button = widgets.Button(\n",
    "    description=\"Ask\", button_style=\"success\", layout=widgets.Layout(width=\"15%\")\n",
    ")\n",
    "output = widgets.Output()\n",
    "input_row = widgets.HBox([question_input, submit_button])\n",
    "display(file_uploader, input_row, output)\n",
    "\n",
    "# --- FILE PROCESSING ---\n",
    "\n",
    "\n",
    "def extract_file_content(change):\n",
    "    global index, chunk_map\n",
    "    chunk_map = []\n",
    "\n",
    "    uploaded_files = file_uploader.value\n",
    "    if not uploaded_files:\n",
    "        return\n",
    "\n",
    "    file_info = uploaded_files[0]\n",
    "    file_name = file_info[\"name\"]\n",
    "    file_type = file_name.split(\".\")[-1].lower()\n",
    "    file_data = file_info[\"content\"]\n",
    "\n",
    "    if file_type == \"pdf\":\n",
    "        reader = PyPDF2.PdfReader(io.BytesIO(file_data))\n",
    "        text = \"\\n\".join(\n",
    "            page.extract_text() for page in reader.pages if page.extract_text()\n",
    "        )\n",
    "    elif file_type in [\"xlsx\", \"xls\"]:\n",
    "        df = pd.read_excel(io.BytesIO(file_data))\n",
    "        text = df.to_csv(index=False)\n",
    "    else:\n",
    "        text = \"‚ùå Unsupported file type.\"\n",
    "\n",
    "    # Split into chunks\n",
    "    docs = splitter.split_documents([Document(page_content=text)])\n",
    "\n",
    "    # Embed and build vector store\n",
    "    texts = [doc.page_content for doc in docs]\n",
    "    chunk_map = texts  # Keep for retrieval\n",
    "    embeddings = embedder.encode(texts, convert_to_numpy=True)\n",
    "\n",
    "    # Build FAISS index\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(embeddings)\n",
    "\n",
    "    with output:\n",
    "        clear_output()\n",
    "        print(f\"‚úÖ File '{file_name}' loaded ‚Äî {len(texts)} chunks embedded.\")\n",
    "        print(\"üìÑ You can now ask questions about the file.\")\n",
    "\n",
    "\n",
    "file_uploader.observe(extract_file_content, names=\"value\")\n",
    "\n",
    "\n",
    "# --- Q&A HANDLER ---\n",
    "\n",
    "\n",
    "def on_button_click(b):\n",
    "    global index, chunk_map\n",
    "\n",
    "    query = question_input.value\n",
    "    if not query.strip():\n",
    "        with output:\n",
    "            print(\"‚ö†Ô∏è Please type a question.\")\n",
    "        return\n",
    "\n",
    "    if index is None or not chunk_map:\n",
    "        with output:\n",
    "            print(\"‚ö†Ô∏è Please upload and process a file first.\")\n",
    "        return\n",
    "\n",
    "    # Embed the question\n",
    "    query_embedding = embedder.encode([query], convert_to_numpy=True)\n",
    "\n",
    "    # Search top 3 similar chunks\n",
    "    k = 3\n",
    "    D, I = index.search(query_embedding, k)\n",
    "\n",
    "    # Build context from top chunks\n",
    "    top_chunks = [chunk_map[i] for i in I[0]]\n",
    "    context = \"\\n---\\n\".join(top_chunks)\n",
    "\n",
    "    # Send to local LLM\n",
    "    prompt = f\"You are reading a document. Here are some parts of it:\\n\\n{context}\\n\\nNow answer this question:\\n{query}\"\n",
    "    response = llm(prompt)\n",
    "\n",
    "    with output:\n",
    "        print(f\"\\nüß† Question: {query}\")\n",
    "        print(f\"üìö Retrieved {k} relevant chunks.\")\n",
    "        print(f\"ü§ñ Answer:\\n{response}\")\n",
    "\n",
    "    question_input.value = \"\"\n",
    "\n",
    "\n",
    "submit_button.on_click(on_button_click)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a70b14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a0fd39fb7c84d86a9b323b67c09aadd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.pdf,.xlsx,.xls', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "upload = widgets.FileUpload(accept=\".pdf,.xlsx,.xls\", multiple=False)\n",
    "display(upload)\n",
    "\n",
    "\n",
    "def check_upload(change):\n",
    "    print(\"Uploader value:\", upload.value)\n",
    "\n",
    "\n",
    "upload.observe(check_upload, names=\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cd8c82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
